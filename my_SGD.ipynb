{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b385fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from model import mf, mfDataset, compute_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e91ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './ml-1m/ratings.dat'\n",
    "batch_size = 1\n",
    "device = torch.device('cuda:0')\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-5\n",
    "epochs = 10\n",
    "embedding_size = 100\n",
    "loss_func = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4013bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dou/anaconda3/envs/catn/lib/python3.7/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users:6040\n",
      "num_items:3952\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, header=None, delimiter='::')\n",
    "x, y = df.iloc[:, :2], df.iloc[:, 2]\n",
    "\n",
    "x_train, x_val_test, y_train, y_val_test = train_test_split(x, y, test_size=0.1)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=0.5)\n",
    "\n",
    "train_dataset = mfDataset(np.array(x_train[0]), np.array(\n",
    "        x_train[1]),  np.array(y_train).astype(np.float32))\n",
    "val_dataset = mfDataset(np.array(x_val[0]), np.array(\n",
    "        x_val[1]),  np.array(y_val).astype(np.float32))\n",
    "test_dataset = mfDataset(np.array(x_test[0]), np.array(\n",
    "        x_test[1]), np.array(y_test).astype(np.float32))\n",
    "\n",
    "train_DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_DataLoader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_DataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mean_rating = df.iloc[:, 2].mean()\n",
    "num_users = max(df[0])+1\n",
    "num_items = max(df[1])+1\n",
    "print(f\"num_users:{num_users-1}\")\n",
    "print(f\"num_items:{num_items-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f040e727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.581564453029317"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e8ee7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0009],\n",
       "        [ 0.0775],\n",
       "        [ 0.0659],\n",
       "        ...,\n",
       "        [-0.0332],\n",
       "        [-0.0676],\n",
       "        [ 0.0711]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create user & item latent factor vectors\n",
    "user_emb = torch.empty(num_users, embedding_size).to(device)\n",
    "user_bias = torch.empty(num_users, 1).to(device)\n",
    "item_emb = torch.empty(num_items, embedding_size).to(device)\n",
    "item_bias = torch.empty(num_items, 1).to(device)\n",
    "\n",
    "nn.init.normal_(user_emb, mean=0, std=0.1)\n",
    "nn.init.normal_(user_bias, mean=0, std=0.1)\n",
    "nn.init.normal_(item_emb, mean=0, std=0.1)\n",
    "nn.init.normal_(item_bias, mean=0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "165b12ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8086], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=torch.tensor([1])\n",
    "U=user_emb[index]\n",
    "I=item_emb[index]\n",
    "U_b=user_bias[index]\n",
    "I_b=item_bias[index]\n",
    "((U*I).sum(1)+U_b+I_b+mean_rating).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2710084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2063])\n",
      "tensor([597])\n",
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "for x_u, x_i, y in train_DataLoader:\n",
    "    print(x_u)\n",
    "    print(x_i)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b6ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dou/anaconda3/envs/catn/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11304/2039807624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mitem_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me_ui\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0muser_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitem_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0muser_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me_ui\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0muser_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mitem_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me_ui\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitem_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training & val\n",
    "for epoch in range(epochs):\n",
    "    # training phase\n",
    "    total_loss, total_len = 0, 0\n",
    "    for x_u, x_i, y in train_DataLoader:\n",
    "        x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\n",
    "        y_pre = compute_rating(user_emb[x_u], item_emb[x_i], user_bias[x_u], item_bias[x_i], mean_rating).to(device)\n",
    "        loss = loss_func(y, y_pre)\n",
    "        e_ui=y-y_pre\n",
    "\n",
    "        # gradient descent\n",
    "        user_emb[x_u] -= learning_rate * 2*(-e_ui*item_emb[x_i] + weight_decay*user_emb[x_u])\n",
    "        item_emb[x_i] -= learning_rate * 2*(-e_ui*user_emb[x_u] + weight_decay*item_emb[x_i])\n",
    "        user_bias[x_u] -= learning_rate * 2*(-e_ui* + weight_decay*user_bias[x_u])\n",
    "        item_bias[x_i] -= learning_rate * 2*(-e_ui* + weight_decay*item_bias[x_i])\n",
    "\n",
    "        total_loss += loss.item()*len(y)\n",
    "        total_len += len(y)\n",
    "    train_loss = total_loss/total_len\n",
    "    \n",
    "    # val phase\n",
    "    labels, predicts = [], []\n",
    "    for x_u, x_i, y in val_DataLoader:\n",
    "        x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\n",
    "        y_pre = compute_rating(user_emb[x_u], item_emb[x_i], user_bias[x_u], item_bias[x_i], mean_rating)\n",
    "        labels.extend(y.tolist())\n",
    "        predicts.extend(y_pre.tolist())\n",
    "    mse = mean_squared_error(np.array(labels), np.array(predicts))\n",
    "\n",
    "    print(\"epoch {}, train loss is {}, val mse is {}\".format(\n",
    "        epoch, train_loss, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "984c10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4842e-44]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.4842e-44])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.empty(1,1, dtype=torch.float32)\n",
    "print(a)\n",
    "a.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b38e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:catn] *",
   "language": "python",
   "name": "conda-env-catn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
