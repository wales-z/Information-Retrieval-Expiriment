{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import libraries or packges that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "import torch.nn as nn\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from model import mf, mfDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Set the training configuration, i.e. hyper-paramters, computing device, dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './ml-1m/ratings.dat'\r\n",
    "batch_size = 2048\r\n",
    "device = torch.device('cuda:0')\r\n",
    "learning_rate = 1e-2\r\n",
    "weight_decay = 1e-5\r\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocessing before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users:6040\n",
      "num_items:3952\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, header=None, delimiter='::')\r\n",
    "x, y = df.iloc[:, :2], df.iloc[:, 2]\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\r\n",
    "\r\n",
    "train_dataset = mfDataset(np.array(x_train[0]), np.array(\r\n",
    "        x_train[1]),  np.array(y_train).astype(np.float32))\r\n",
    "test_dataset = mfDataset(np.array(x_test[0]), np.array(\r\n",
    "        x_test[1]), np.array(y_test).astype(np.float32))\r\n",
    "\r\n",
    "train_DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
    "test_DataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "mean_rating = df.iloc[:, 2].mean()\r\n",
    "num_users = max(df[0])+1\r\n",
    "num_items = max(df[1])+1\r\n",
    "print(f\"num_users:{num_users-1}\")\r\n",
    "print(f\"num_items:{num_items-1}\")\r\n",
    "\r\n",
    "# generate two mf model objects: model using pytorch auto-gradient，model_my_SGD using my implementation of gradient descent\r\n",
    "model = mf(num_users, num_items, mean_rating).to(device)\r\n",
    "model_my_SGD = mf(num_users, num_items, mean_rating).to(device)\r\n",
    "\r\n",
    "# l2 normalization\r\n",
    "optimizer = torch.optim.SGD(\r\n",
    "        params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\r\n",
    "\r\n",
    "# optimizer = torch.optim.Adam(\r\n",
    "#         params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\r\n",
    "\r\n",
    "loss_func = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training and evaluation：pytorch\r\n",
    "optimization method: Stochastic Gradient Descent (SGD) or Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch auto-gradient round\n",
      "__________________________________________________________________\n",
      "epoch 0, train loss is 14.090911655923273, val mse is 14.039846853853131\n",
      "epoch 1, train loss is 14.090343705332716, val mse is 14.03983020377541\n",
      "epoch 2, train loss is 14.089775769018859, val mse is 14.03981354223406\n",
      "epoch 3, train loss is 14.08920782449664, val mse is 14.03979657519331\n",
      "epoch 4, train loss is 14.088639745797161, val mse is 14.039779221357968\n",
      "epoch 5, train loss is 14.08807135681568, val mse is 14.039761120212809\n",
      "epoch 6, train loss is 14.08750229013797, val mse is 14.039742380041316\n",
      "epoch 7, train loss is 14.086932756371853, val mse is 14.039722573208241\n",
      "epoch 8, train loss is 14.086362413385023, val mse is 14.039701701566976\n",
      "epoch 9, train loss is 14.08579094435252, val mse is 14.039679737991243\n",
      "epoch 10, train loss is 14.085218370644903, val mse is 14.039656303188956\n",
      "epoch 11, train loss is 14.084644486811584, val mse is 14.039631166920934\n",
      "epoch 12, train loss is 14.084069133262009, val mse is 14.039604166271884\n",
      "epoch 13, train loss is 14.083491986899475, val mse is 14.039575432225469\n",
      "epoch 14, train loss is 14.082913148910999, val mse is 14.039544743946843\n",
      "__________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch auto-gradient round\")\r\n",
    "print(\"__________________________________________________________________\")\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    # training phase\r\n",
    "\r\n",
    "    model.train()\r\n",
    "    total_loss, total_len = 0, 0\r\n",
    "    for x_u, x_i, y in train_DataLoader:\r\n",
    "        x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\r\n",
    "        y_pre, p_u, q_i = model(x_u, x_i)\r\n",
    "        loss = loss_func(y_pre, y)\r\n",
    "\r\n",
    "        # auto gradient computing and gradient descent based on pytorch\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        total_loss += loss.item()*len(y)\r\n",
    "        total_len += len(y)\r\n",
    "    train_loss = total_loss/total_len\r\n",
    "\r\n",
    "\r\n",
    "    # evaluation phase\r\n",
    "    model.eval()\r\n",
    "    labels, predicts = [], []\r\n",
    "    with torch.no_grad():\r\n",
    "        for x_u, x_i, y in test_DataLoader:\r\n",
    "            x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\r\n",
    "            y_pre, p_u, q_i = model(x_u, x_i)\r\n",
    "            labels.extend(y.tolist())\r\n",
    "            predicts.extend(y_pre.tolist())\r\n",
    "    mse = mean_squared_error(np.array(labels), np.array(predicts))\r\n",
    "\r\n",
    "    print(\"epoch {}, train loss is {}, val mse is {}\".format(\r\n",
    "        epoch, train_loss, mse))\r\n",
    "print(\"__________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training and evaluation：my implemention for gradient descent\r\n",
    "optimization method: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新生成torch的dataloader部分\r\n",
    "train_dataset = mfDataset(np.array(x_train[0]), np.array(\r\n",
    "        x_train[1]),  np.array(y_train).astype(np.float32))\r\n",
    "test_dataset = mfDataset(np.array(x_test[0]), np.array(\r\n",
    "        x_test[1]), np.array(y_test).astype(np.float32))\r\n",
    "\r\n",
    "train_DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
    "test_DataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "print(\"my implementation round\")\r\n",
    "print(\"__________________________________________________________________\")\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    # training phase\r\n",
    "    model.train()\r\n",
    "    total_loss, total_len = 0, 0\r\n",
    "    for x_u, x_i, y in train_DataLoader:\r\n",
    "        x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\r\n",
    "        y_pre, p_u, q_i = model_my_SGD(x_u, x_i)\r\n",
    "        # l2 normalization\r\n",
    "        loss = loss_func(y_pre, y) + weight_decay * (torch.sum(torch.pow(p_u, 2)) + torch.sum(torch.pow(q_i, 2)))\r\n",
    "\r\n",
    "        # my implementation for gradient descent of mf model\r\n",
    "        e_ui = (y - y_pre).unsqueeze(1)\r\n",
    "        model_my_SGD.my_gradient_descent(x_u, x_i, e_ui, learning_rate, weight_decay)\r\n",
    "\r\n",
    "        total_loss += loss.item()*len(y)\r\n",
    "        total_len += len(y)\r\n",
    "    train_loss = total_loss/total_len\r\n",
    "\r\n",
    "    # evaluation phase\r\n",
    "    model.eval()\r\n",
    "    labels, predicts = [], []\r\n",
    "    with torch.no_grad():\r\n",
    "        for x_u, x_i, y in test_DataLoader:\r\n",
    "            x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\r\n",
    "            y_pre, p_u, q_i = model(x_u, x_i)\r\n",
    "            labels.extend(y.tolist())\r\n",
    "            predicts.extend(y_pre.tolist())\r\n",
    "    mse = mean_squared_error(np.array(labels), np.array(predicts))\r\n",
    "\r\n",
    "    print(\"epoch {}, train loss is {}, val mse is {}\".format(\r\n",
    "        epoch, train_loss, mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python376jvsc74a57bd051a9663a131f1b5758c45b97a2d6917c8ae86b33e231c3733631cbc7265cfc89"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 5
}