{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import libraries or packges that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from model import mf, mfDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Set the training configuration, i.e. hyper-paramters, computing device, dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './ml-1m/ratings.dat'\n",
    "batch_size = 2048\n",
    "device = torch.device('cuda:0')\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-5\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocessing before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dou/anaconda3/envs/catn/lib/python3.7/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users:6040\n",
      "num_items:3952\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, header=None, delimiter='::')\n",
    "x, y = df.iloc[:, :2], df.iloc[:, 2]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "train_dataset = mfDataset(np.array(x_train[0]), np.array(\n",
    "        x_train[1]),  np.array(y_train).astype(np.float32))\n",
    "test_dataset = mfDataset(np.array(x_test[0]), np.array(\n",
    "        x_test[1]), np.array(y_test).astype(np.float32))\n",
    "\n",
    "train_DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_DataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mean_rating = df.iloc[:, 2].mean()\n",
    "num_users = max(df[0])+1\n",
    "num_items = max(df[1])+1\n",
    "print(f\"num_users:{num_users-1}\")\n",
    "print(f\"num_items:{num_items-1}\")\n",
    "\n",
    "# generate two mf model objects: model using pytorch auto-gradient，model_my_SGD using my implementation of gradient descent\n",
    "model = mf(num_users, num_items, mean_rating).to(device)\n",
    "model_my_SGD = mf(num_users, num_items, mean_rating).to(device)\n",
    "\n",
    "# l2 normalization\n",
    "optimizer = torch.optim.SGD(\n",
    "        params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#         params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "loss_func = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training and evaluation：my implemention for gradient descent\n",
    "optimization method: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([3.5816], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[-0.0253,  0.1679, -0.0207,  ...,  0.0874,  0.1041, -0.1503],\n",
       "         [-0.0715,  0.0070, -0.0165,  ..., -0.0576, -0.0096,  0.0791],\n",
       "         [-0.0632,  0.0122, -0.1540,  ..., -0.0274, -0.1025, -0.0105],\n",
       "         ...,\n",
       "         [ 0.1010,  0.1880,  0.1345,  ...,  0.0947, -0.0709, -0.0941],\n",
       "         [ 0.0536, -0.1759, -0.1022,  ...,  0.0584,  0.0765, -0.1517],\n",
       "         [-0.0850, -0.0641, -0.0068,  ...,  0.0331, -0.1700, -0.0363]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0075],\n",
       "         [ 0.1644],\n",
       "         [-0.1334],\n",
       "         ...,\n",
       "         [ 0.0620],\n",
       "         [ 0.0478],\n",
       "         [ 0.0403]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1983, -0.0945,  0.0466,  ...,  0.0370,  0.2087,  0.0359],\n",
       "         [ 0.0436,  0.1404, -0.0421,  ..., -0.0293,  0.1588,  0.1411],\n",
       "         [-0.0680, -0.1402,  0.0332,  ..., -0.0433,  0.1388,  0.0046],\n",
       "         ...,\n",
       "         [ 0.0307,  0.1230,  0.0185,  ...,  0.2043, -0.0100, -0.0546],\n",
       "         [ 0.0444, -0.1802, -0.0417,  ...,  0.0087,  0.0356, -0.0280],\n",
       "         [-0.1389, -0.0258,  0.0477,  ..., -0.0181,  0.0357, -0.0068]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0260],\n",
       "         [-0.1481],\n",
       "         [-0.1615],\n",
       "         ...,\n",
       "         [-0.0253],\n",
       "         [-0.1011],\n",
       "         [ 0.0242]], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_my_SGD.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training and evaluation：pytorch\n",
    "optimization method: Stochastic Gradient Descent (SGD) or Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my implementation round\n",
      "__________________________________________________________________\n",
      "tensor([[-0.1505,  0.0789, -0.0449,  ...,  0.0254, -0.0016, -0.1366],\n",
      "        [-0.1467, -0.0782, -0.0597,  ..., -0.0115,  0.0028,  0.0806],\n",
      "        [-0.0667, -0.0718, -0.1231,  ...,  0.0016, -0.2394, -0.0391],\n",
      "        ...,\n",
      "        [-0.0357,  0.3117,  0.1836,  ..., -0.1788, -0.0315,  0.0518],\n",
      "        [-0.0494,  0.0659,  0.1187,  ...,  0.0187, -0.0242, -0.0659],\n",
      "        [ 0.0240, -0.0067,  0.0934,  ..., -0.0267, -0.0526, -0.0227]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24441/3199942371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# my implementation for gradient descent of mf model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0me_ui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmodel_my_SGD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_ui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zhouxiaolong/Information-Retrieval-Expiriment/model.py\u001b[0m in \u001b[0;36mmy_gradient_descent\u001b[0;34m(self, u_id, i_id, e_ui, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0me_ui\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mI\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0me_ui\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mU\u001b[0m \u001b[0m利用了\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0m点积的广播机制\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         '''\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mU_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "print(\"my implementation round\")\n",
    "print(\"__________________________________________________________________\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # training phase\n",
    "    model.train()\n",
    "    total_loss, total_len = 0, 0\n",
    "    for x_u, x_i, y in train_DataLoader:\n",
    "        x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\n",
    "        y_pre, p_u, q_i = model_my_SGD(x_u, x_i)\n",
    "        # l2 normalization\n",
    "        loss = loss_func(y_pre, y) + weight_decay * (torch.sum(torch.pow(p_u, 2)) + torch.sum(torch.pow(q_i, 2)))\n",
    "\n",
    "        # my implementation for gradient descent of mf model\n",
    "        e_ui = (y - y_pre).unsqueeze(1)\n",
    "        model_my_SGD.my_gradient_descent(x_u, x_i, e_ui, learning_rate, weight_decay)\n",
    "\n",
    "        total_loss += loss.item()*len(y)\n",
    "        total_len += len(y)\n",
    "    train_loss = total_loss/total_len\n",
    "\n",
    "    # evaluation phase\n",
    "    model.eval()\n",
    "    labels, predicts = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_u, x_i, y in test_DataLoader:\n",
    "            x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\n",
    "            y_pre, p_u, q_i = model(x_u, x_i)\n",
    "            labels.extend(y.tolist())\n",
    "            predicts.extend(y_pre.tolist())\n",
    "    mse = mean_squared_error(np.array(labels), np.array(predicts))\n",
    "\n",
    "    print(\"epoch {}, train loss is {}, val mse is {}\".format(\n",
    "        epoch, train_loss, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pytorch auto-gradient round\")\n",
    "print(\"__________________________________________________________________\")\n",
    "\n",
    "# 重新生成torch的dataloader部分\n",
    "train_dataset = mfDataset(np.array(x_train[0]), np.array(\n",
    "        x_train[1]),  np.array(y_train).astype(np.float32))\n",
    "test_dataset = mfDataset(np.array(x_test[0]), np.array(\n",
    "        x_test[1]), np.array(y_test).astype(np.float32))\n",
    "\n",
    "train_DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_DataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # training phase\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_len = 0, 0\n",
    "    for x_u, x_i, y in train_DataLoader:\n",
    "        x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\n",
    "        y_pre, p_u, q_i = model(x_u, x_i)\n",
    "        loss = loss_func(y_pre, y)\n",
    "\n",
    "        # auto gradient computing and gradient descent based on pytorch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()*len(y)\n",
    "        total_len += len(y)\n",
    "    train_loss = total_loss/total_len\n",
    "\n",
    "\n",
    "    # evaluation phase\n",
    "    model.eval()\n",
    "    labels, predicts = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_u, x_i, y in test_DataLoader:\n",
    "            x_u, x_i, y = x_u.to(device), x_i.to(device), y.to(device)\n",
    "            y_pre, p_u, q_i = model(x_u, x_i)\n",
    "            labels.extend(y.tolist())\n",
    "            predicts.extend(y_pre.tolist())\n",
    "    mse = mean_squared_error(np.array(labels), np.array(predicts))\n",
    "\n",
    "    print(\"epoch {}, train loss is {}, val mse is {}\".format(\n",
    "        epoch, train_loss, mse))\n",
    "print(\"__________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=nn.Embedding(5,1)\n",
    "b=nn.Embedding(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a(torch.tensor([1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3988],\n",
       "        [-0.7444]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2928],\n",
       "        [-0.0324]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to function call (2206252653.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_24441/2206252653.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a(torch.tensor([1,2]))=a(torch.tensor([1,2]))-torch.tensor([1,1])\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"
     ]
    }
   ],
   "source": [
    "a(torch.tensor([1,2]))=a(torch.tensor([1,2]))-torch.tensor([1,1])\n",
    "print(a(torch.tensor([1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5695e-43, 1.5554e-43, 1.5975e-43])\n",
      "tensor([0.0254, 0.0013, 0.1023])\n"
     ]
    }
   ],
   "source": [
    "a=torch.empty(3)\n",
    "print(a)\n",
    "nn.init.normal_(a, mean=0, std=0.1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0254, 0.0013, 0.1023])\n",
      "tensor([0.0254, 0.1023])\n"
     ]
    }
   ],
   "source": [
    "b=torch.gather(a, dim=0, index=torch.tensor([0,2]))\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0254, 0.0013, 0.1023])\n"
     ]
    }
   ],
   "source": [
    "b+=torch.tensor([1,1])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
